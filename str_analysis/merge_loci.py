import argparse
import collections
import ijson
import intervaltree
import gzip
import simplejson as json
import os
import re
import tqdm

from str_analysis.utils.canonical_repeat_unit import compute_canonical_motif
from str_analysis.utils.eh_catalog_utils import parse_motifs_from_locus_structure, convert_json_records_to_bed_format_tuples
from str_analysis.utils.file_utils import open_file, file_exists
from str_analysis.utils.misc_utils import parse_interval

REQUIRED_OUTPUT_FIELDS = {"LocusId", "ReferenceRegion", "LocusStructure", "VariantType"}
SEPARATOR_FOR_MULTIPLE_SOURCES = " ||| "

def parse_args():
    parser = argparse.ArgumentParser(description="Combines two or more repeat catalogs into a single catalog. Loci "
        "that have similar motifs and overlap either by more than the given threshold or by at least two repeats of "
        "the larger motif among the two can be merged or discarded.")
    parser.add_argument("-f", "--overlap-fraction", default=0.66, type=float, help="The minimum overlap for two loci "
        "to be considered as the same locus (assuming they are specified as having the same normalized motif). "
        "This is similar to the -f argument for 'bedtools intersect'.")
    parser.add_argument("--motif-match-type", choices=("canonical", "length"), default="canonical", help="The type of "
        "motif match to use when comparing loci. 'canonical' will require the canonical motifs to match. "
        "'length' will only require the motifs to be the same length.")
    parser.add_argument("--add-source-field", action="store_true", help="If specified, then a Source field will be "
        "added to the output catalog to specify the filename of the original source catalog of a given locus. This "
        "requires the output format to be set to JSON.")
    parser.add_argument("--add-found-in-fields", action="store_true", help="If specified, then 'FoundIn<CatalogName>' "
        "fields will added to the output catalog to indicate which input catalogs contain this locus. "
        "This requires the output format to be set to JSON.")
    parser.add_argument("--discard-extra-fields-from-input-catalogs", action="store_true", help="If specified, then "
        "extra fields from the input catalogs will not be copied to to the output catalog. This requires the output format "
        "to be set to JSON.")
    parser.add_argument("--merge-type", choices=("union", "intersection"), default="union", help="If 'union', then "
        "non-duplicate loci from all input catalogs will be added to the output catalog. If 'intersection', then only "
        "loci that are present in all input catalogs will be included in the output")
    parser.add_argument("--overlapping-loci-action",
        choices=("keep-first", "keep-last", "keep-both", "keep-narrow", "keep-wider", "merge"),
        default="keep-first", help="When two loci overlap and have the same canonical motif, this option specifies "
        "which locus to keep in the output catalog. 'keep-first' keeps the first locus, 'keep-last' keeps the last "
        "locus, 'keep-both' keeps both loci, 'keep-narrow' keeps the locus with the narrower interval, 'keep-wider' "
        "keeps the locus with the wider interval, and 'merge' merges the two loci into a single locus.")
    parser.add_argument("-m", "--merge-adjacent-loci-with-same-motif", action="store_true",
        help="A repeat catalog generated by using a tool like TandemRepeatFinder to find all pure repeats in a "
        "reference genome provides a good starting point for a repeat catalog. However, the presence of a single base "
        "pair interruption in a reference repeat sequence will cause that repeat locus to be split into 2 adjacent "
        "loci with the same motif (under cyclic shift). This option enables detection of these adjacent loci (separated "
        "by 1 base pair and having the same motif) and merges them into a single locus in the output catalog.")
    parser.add_argument("--output-format", choices=("JSON", "BED"), help="Output file format. If not specified, both "
                                                                         "a JSON and a BED file will be generated.")
    parser.add_argument("--output-prefix", help="Output filename prefix")
    parser.add_argument("--verbose", action="store_true", help="If specified, then print more stats")
    parser.add_argument("--verbose-overlaps", action="store_true", help="If specified, print out overlapping definitions"
                        "that have similar motifs but different boundaries")
    parser.add_argument("--show-progress-bar", action="store_true", help="Show a progress bar")
    parser.add_argument("--outer-join-overlap-table-min-sources", type=int, default=1, help="The minimum number of "
                        "input catalogs that a locus must be found in to be included in the outer join overlap table")
    parser.add_argument("--write-outer-join-table", action="store_true", help="If specified, output an "
                        ".outer_join_overlap_table.tsv.gz which reports which loci are present in which input catalogs")
    parser.add_argument("--write-merge-stats-tsv", action="store_true", help="If specified, output a .merge_stats.tsv")
    parser.add_argument("--write-bed-files-with-unique-loci", action="store_true", help="If specified, then for every "
                        "input catalog except the first one, this script will output a BED file that contains the new "
                        "loci introduced by that catalog. This is useful for troubleshooting catalogs and "
                        "understanding the differences between them.")
    parser.add_argument("variant_catalog_json_or_bed", nargs="+", help="Paths of two or more repeat catalogs "
        "in JSON or BED format. For BED format, the chrom, start, and end should represent the repeat "
        "interval in 0-based coordinates, and the name field (column #4) should be the repeat unit. The order in which "
        "the catalogs are specified is important as it determines the behavior of the --overlapping-loci-action. "
        "By default all loci from the 1st catalog will be included in the output catalog, then all loci from the 2nd "
        "catalog that aren't represented in the 1st catalog and so on. If --add-found-in-fields is specified, each "
        "path should be preceded by a name for that catalog, followed by ':', and then the path. "
        "For example: catalog1:/path/to/filename.bed. This name will be used in the Source field and as a key specifying "
        "which loci are represented in that catalog.")

    args = parser.parse_args()

    if args.merge_adjacent_loci_with_same_motif and (args.write_outer_join_table or args.add_found_in_fields):
        parser.error("--merge-adjacent-loci-with-same-motif can not be used with --write-outer-join-table or --add-found-in-fields")
    if args.merge_adjacent_loci_with_same_motif and args.merge_type == "intersection":
        parser.error("--merge-adjacent-loci-with-same-motif can not be used with --merge-type 'intersection'")
    if args.overlapping_loci_action == "merge" and (args.write_outer_join_table or args.add_found_in_fields):
        parser.error("--overlapping-loci-action 'merge' can not be used with --write-outer-join-table or --add-found-in-fields")

    if args.output_format == "BED":
        if args.add_source_field:
            parser.error("The --add-source-field option requires --output-format JSON to be specified")
        if args.add_found_in_fields:
            parser.error("The --add-found-in-fields option requires --output-format JSON to be specified")
        if args.discard_extra_fields_from_input_catalogs:
            parser.error("The --discard-extra-fields-from-input-catalogs option requires --output-format JSON to be specified")

    paths = parse_variant_catalog_paths_arg(args, parser)

    return args, paths


def parse_variant_catalog_paths_arg(args, argparser):
    paths = []
    catalog_names = set()
    for path in args.variant_catalog_json_or_bed:
        if path.count(":") > 1:
            argparser.error(f"{path} contains more than 1 colon")
        elif args.add_found_in_fields and ":" not in path:
            argparser.error("When --add-found-in-fields is specified, each path should be preceded by a catalog name "
                            "followed by ':' and then the path. For example: catalog1:/path/to/filename.bed")

        if ":" in path:
            catalog_name, path = path.split(":")
        else:
            catalog_name = os.path.basename(path)

        if catalog_name in catalog_names:
            argparser.error(f"Duplicate catalog name: {catalog_name}")
        else:
            catalog_names.add(catalog_name)

        if not file_exists(path):
            argparser.error(f"{path} not found")

        if SEPARATOR_FOR_MULTIPLE_SOURCES in catalog_name:
            argparser.error(f"Catalog name {catalog_name} must not contain this character sequence: '{SEPARATOR_FOR_MULTIPLE_SOURCES}'")

        if any(path.endswith(suffix) for suffix in [".bed", ".bed.gz", ".bed.bgz"]):
            paths.append((catalog_name, path, "BED"))
        elif any(path.endswith(suffix) for suffix in [".json", ".json.gz"]):
            paths.append((catalog_name, path, "JSON"))
        else:
            argparser.error(f"Unrecognized file extension: {path}")

    return paths


def get_variant_catalog_iterator(
        variant_catalog_json_or_bed,
        file_type,
        discard_extra_fields_from_input_catalogs=False,
        verbose=False,
        show_progress_bar=False):
    """Takes the path of a JSON or BED file and returns an iterator over variant catalog records parsed from that file.

    Args:
        intervaltrees (dict): a dictionary that maps chromosome names to IntervalTrees
        variant_catalog_json_or_bed (str): path to a JSON or BED file containing variant catalog records
        file_type (str): either "JSON" or "BED"
        discard_extra_fields_from_input_catalogs (bool): If False, then only the required fields will be kept from the
            input variant catalog and any extra fields will be discarded.
        verbose (bool): If True, add a progress bar
        show_progress_bar (bool): If True, then show a progress bar
    """
    if verbose:
        print(f"Parsing {variant_catalog_json_or_bed}")

    if file_type == "JSON":
        is_json = True
    elif file_type == "BED":
        is_json = False
    else:
        raise ValueError(f"Unrecognized file type: {file_type}")

    try:
        if is_json:
            with open_file(variant_catalog_json_or_bed, is_text_file=True) as f:
                file_iterator = ijson.items(f, "item", use_float=True)
                if show_progress_bar:
                    file_iterator = tqdm.tqdm(file_iterator, unit=" records", unit_scale=True)

                for record_i, record in enumerate(file_iterator):
                    missing_keys = REQUIRED_OUTPUT_FIELDS - record.keys()
                    if missing_keys:
                        raise ValueError(f"Record #{record_i+1} in {variant_catalog_json_or_bed} is missing required "
                                         f"field(s): {missing_keys}")

                    if discard_extra_fields_from_input_catalogs:
                        record = {k: record[k] for k in REQUIRED_OUTPUT_FIELDS}

                    if isinstance(record["ReferenceRegion"], list):
                        reference_region_count = len(record["ReferenceRegion"])
                        start_0based, end_1based = None, None
                        for reference_region in record["ReferenceRegion"]:
                            unmodified_chrom, current_start_0based, current_end_1based = parse_interval(reference_region)
                            if start_0based is None or current_start_0based < start_0based:
                                start_0based = current_start_0based
                            if end_1based is None or current_end_1based > end_1based:
                                end_1based = current_end_1based
                    else:
                        reference_region_count = 1
                        unmodified_chrom, start_0based, end_1based = parse_interval(record["ReferenceRegion"])

                    motifs = parse_motifs_from_locus_structure(record["LocusStructure"])
                    if len(motifs) != reference_region_count:
                        print(f"ERROR: {variant_catalog_json_or_bed} record {record_i+1:,d}: locus structure "
                              f"{record['LocusStructure']} contains a different number of motifs ({len(motifs)}) than the "
                              f"number of reference regions ({reference_region_count}): {record['ReferenceRegion']}. "
                              f"Skipping...")
                        continue

                    yield unmodified_chrom, start_0based, end_1based, record

        else:
            with open_file(variant_catalog_json_or_bed, is_text_file=True) as file_iterator:
                if show_progress_bar:
                    file_iterator = tqdm.tqdm(file_iterator, unit=" records", unit_scale=True)

                for line_i, line in enumerate(file_iterator):
                    fields = line.strip().split("\t")
                    unmodified_chrom = fields[0]
                    chrom = unmodified_chrom.replace("chr", "")
                    start_0based = int(fields[1])
                    end_1based = int(fields[2])
                    if "(" in fields[3] or ")" in fields[3]:
                        motifs = parse_motifs_from_locus_structure(fields[3])
                        if len(motifs) != 1:
                            filename = os.path.basename(variant_catalog_json_or_bed)
                            print(f"WARNING: {filename} line #{line_i+1:,d}: {chrom}:{start_0based }-{end_1based} "
                                  f"locus structure {fields[3]} contains more than one motif. Skipping...")
                            continue

                        motif = motifs[0]
                    else:
                        motif = fields[3]

                    record = {
                        "LocusId": f"{chrom}-{start_0based}-{end_1based}-{motif}",
                        "ReferenceRegion": f"{unmodified_chrom}:{start_0based}-{end_1based}",
                        "LocusStructure": f"({motif})*",
                        "VariantType": "Repeat",
                    }

                    yield unmodified_chrom, start_0based, end_1based, record
    except Exception as e:
        raise ValueError(f"Error parsing {variant_catalog_json_or_bed}: {e}")

def check_for_sufficient_overlap_and_motif_match(
    existing_interval, new_interval, counters=None, min_overlap_fraction=0.66, motif_match_type="canonical"):

    existing_record = existing_interval.data
    new_record = new_interval.data
    overlap_size = existing_interval.overlap_size(new_interval)

    # if the new record overlaps an existing record by less than the minimum overlap fraction, then it's not
    # considered a duplicate of the existing record
    sufficient_overlap_size = overlap_size >= min_overlap_fraction * new_interval.length() or \
                              overlap_size >= min_overlap_fraction * existing_interval.length()

    if isinstance(new_record["ReferenceRegion"], list) or isinstance(existing_record["ReferenceRegion"], list):
        if not sufficient_overlap_size:
            return None

        if new_record["LocusStructure"] == existing_record["LocusStructure"]:
            if counters is not None: counters[f"overlapped an existing locus by at least {100*min_overlap_fraction}% " \
                                             f"and had the exact same LocusStructure"] += 1
            return existing_interval

        return None

    # check if the existing record's canonical motif is the same as the canonical motif of the new record
    existing_record_motifs = parse_motifs_from_locus_structure(existing_record["LocusStructure"])
    if len(existing_record_motifs) != 1:
        raise ValueError(f"Unexpected LocusStructure in {existing_record}.")

    try:
        existing_record_canonical_motif = compute_canonical_motif(
            existing_record_motifs[0], include_reverse_complement=False)
    except Exception as e:
        raise ValueError(f"Error computing canonical motif for {existing_record}: {e}")


    # compute the new record's canonical motif if it hasn't been computed already
    new_record_motifs = parse_motifs_from_locus_structure(new_record["LocusStructure"])
    if len(new_record_motifs) != 1:
        raise ValueError(f"Unexpected LocusStructure in {new_record}.")

    try:
        new_record_canonical_motif = compute_canonical_motif(
            new_record_motifs[0], include_reverse_complement=False)
    except Exception as e:
        raise ValueError(f"Error computing canonical motif for {new_record}: {e}")

    if len(existing_record_canonical_motif) > len(new_record_canonical_motif):
        longer_motif = existing_record_canonical_motif
    else:
        longer_motif = new_record_canonical_motif

    if not sufficient_overlap_size and overlap_size < 2*len(longer_motif):
        return None

    assert motif_match_type in ("canonical", "length")

    if motif_match_type == "canonical" and existing_record_canonical_motif == new_record_canonical_motif:
        if counters is not None: counters[f"overlapped an existing locus by at least {100*min_overlap_fraction}% " \
                                          f"and had the same canonical motif"] += 1
        return existing_interval
    elif motif_match_type == "length" and len(existing_record_canonical_motif) == len(new_record_canonical_motif):
        if counters is not None: counters[f"overlapped an existing locus by at least {100*min_overlap_fraction}% " \
                                          f"and had the same motif length"] += 1
        return existing_interval
    else:
        if len(existing_record_canonical_motif) <= len(new_record_canonical_motif):
            short_motif, long_motif = existing_record_canonical_motif, new_record_canonical_motif
        elif len(existing_record_canonical_motif) > len(new_record_canonical_motif):
            short_motif, long_motif = new_record_canonical_motif, existing_record_canonical_motif

        expanded_motif = short_motif * (1 + len(long_motif)//len(short_motif))
        if expanded_motif[:len(long_motif)] == long_motif:
            if counters is not None: counters[f"overlapped an existing locus by at least {100*min_overlap_fraction}% " \
                                              f"and one motif was contained within the other"] += 1
            return existing_interval

    return None


def add_variant_catalog_to_interval_trees(
        catalog_name,
        variant_catalog_json_or_bed,
        file_type,
        interval_trees,
        outer_join_overlap_table=None,
        overlapping_loci_action="keep-first",
        min_overlap_fraction=0.01,
        motif_match_type="canonical",
        add_source_field=False,
        discard_extra_fields_from_input_catalogs=False,
        stats=None,
        verbose=False,
        verbose_overlaps=False,
        show_progress_bar=False,
        write_bed_files_with_unique_loci=False,
        output_prefix=None,
):
    """Parses the the given input variant catalog and adds any new unique records to the IntervalTrees.

    Args:
        catalog_name (str): catalog name (preferably in PascalCase)
        variant_catalog_json_or_bed (str): path to a JSON or BED file containing input variant catalog records
        file_type (str): either "JSON" or "BED"
        interval_trees (dict): a dictionary that maps chromosome names to IntervalTree objects for overlap detection
        outer_join_overlap_table (dict): a dictionary that maps locus IDs to a dictionary of catalog name to locus presence
        for each locus found in any catalog, maps its LocusId to a dictionary of catalog name to locus presence
        add_source_field (bool): If True, then the source file path will be added to each record as a new "Source" field
        discard_extra_fields_from_input_catalogs (bool): If False, then only the required fields will be kept in each input
            variant catalog record and any extra fields will be discarded.
        stats (dict): dictionary for tracking merge stats
        verbose (bool): If True, then print more stats about the number of records added to the output catalog
        verbose_overlaps (bool): If True, print info about loci that overlap and have similar motifs, but different bondaries.
        show_progress_bar (bool): Whether to show a progress bar
        write_bed_files_with_unique_loci (bool): If True, then write a BED file of loci not seen in previous catalogs.
        output_prefix (str): Output filename prefix
    """
    if verbose:
        print("- "*60)

    variant_catalog_filename = os.path.basename(variant_catalog_json_or_bed)
    counters = collections.defaultdict(int)
    unique_loci = set()
    for unmodified_chrom, start_0based, end_1based, new_record in get_variant_catalog_iterator(
            variant_catalog_json_or_bed, file_type,
            discard_extra_fields_from_input_catalogs=discard_extra_fields_from_input_catalogs,
            verbose=verbose, show_progress_bar=show_progress_bar):

        new_interval = intervaltree.Interval(start_0based, end_1based, data=new_record)

        chrom = unmodified_chrom.replace("chr", "")

        if add_source_field:
            new_record["Source"] = catalog_name
        new_record["ChromStartEndLocusStruct"] = f"{chrom}:{start_0based}-{end_1based} {new_record['LocusStructure']}"
        if outer_join_overlap_table is not None:
            outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]][catalog_name] = "Yes"

        # check for overlap with existing loci
        counters["total"] += 1
        existing_interval = None
        discard_new = False
        remove_existing = False
        overlapping_intervals = interval_trees[chrom].overlap(start_0based, end_1based)
        for overlapping_interval in overlapping_intervals:
            overlap_size = overlapping_interval.overlap_size(start_0based, end_1based)

            existing_interval = check_for_sufficient_overlap_and_motif_match(
                overlapping_interval, new_interval, counters=counters, min_overlap_fraction=min_overlap_fraction,
                motif_match_type=motif_match_type)
            if existing_interval is not None:
                break

        if existing_interval is not None:
            # handle results if overlap was found
            existing_record = existing_interval.data

            if overlapping_loci_action == "keep-first":
                discard_new = True
            elif overlapping_loci_action == "keep-last":
                remove_existing = True
            elif overlapping_loci_action == "keep-both":
                pass
            elif overlapping_loci_action == "keep-narrow":
                if (end_1based - start_0based) >= (existing_interval.end - existing_interval.begin):
                    discard_new = True
                else:
                    remove_existing = True
            elif overlapping_loci_action == "keep-wider":
                if (end_1based - start_0based) <= (existing_interval.end - existing_interval.begin):
                    discard_new = True
                else:
                    remove_existing = True
            elif overlapping_loci_action == "merge":
                stats[variant_catalog_filename]["merged"] += 1
                remove_existing = True
                min_start_0based = min(start_0based, existing_interval.begin)
                max_end_1based = max(end_1based, existing_interval.end)
                existing_record_locus_structure = existing_record["LocusStructure"]
                existing_record_motifs = parse_motifs_from_locus_structure(existing_record_locus_structure)
                if len(current_motifs) != 1:
                    raise ValueError(f"Unexpected LocusStructure in {existing_record}.")
                existing_record_motif  = existing_record_motifs[0]
                new_record = {
                    "LocusId": f"{chrom}-{min_start_0based}-{max_end_1based}-{existing_record_motif}",
                    "ReferenceRegion": f"{unmodified_chrom}:{min_start_0based}-{max_end_1based}",
                    "LocusStructure": existing_record_locus_structure,
                    "VariantType": existing_record["VariantType"],
                }
                if add_source_field:
                    new_record["Source"] = existing_record["Source"] + SEPARATOR_FOR_MULTIPLE_SOURCES + catalog_name
            else:
                raise ValueError(f"Unexpected overlapping_loci_action value: {overlapping_loci_action}")

            if verbose_overlaps:
                if new_record["ReferenceRegion"] != existing_record["ReferenceRegion"]:
                    print("="*100)
                    _, existing_start, existing_end = parse_interval(existing_record["ReferenceRegion"])
                    _, new_start, new_end = parse_interval(new_record["ReferenceRegion"])
                    existing_size = existing_end - existing_start
                    new_size = new_end - new_start
                    size_comparison = f"includes {existing_size - new_size} more bases than" if existing_size > new_size \
                        else "is the same size as" if existing_size == new_size else \
                        f"includes {new_size - existing_size} fewer bases than"
                    print(f"Existing record:", existing_record["ReferenceRegion"], existing_record["LocusStructure"], size_comparison, "the new record")
                    print(f"     New record:", new_record["ReferenceRegion"], new_record["LocusStructure"])
                    print(f"         Action:", "Replacing existing record with new record " if not discard_new and remove_existing else (
                        "Discarding new record" if discard_new else "Adding new record"
                    ))

            # handle overlapping records in the outer-join tables
            if outer_join_overlap_table is not None:
                for overlapping_interval in overlapping_intervals:
                    overlapping_record = overlapping_interval.data
                    if overlapping_record["Source"] == catalog_name:
                        continue

                    they_match = check_for_sufficient_overlap_and_motif_match(
                        overlapping_interval, new_interval, min_overlap_fraction=min_overlap_fraction,
                        motif_match_type=motif_match_type)
                    if not they_match:
                        continue

                    def set_value_if_not_yes(dictionary, key, value, log=None):
                        if dictionary.get(key) != "Yes":
                            dictionary[key] = value
                            if log:
                                print(log, f" setting to {value}")

                    #existing_record_motifs = parse_motifs_from_locus_structure(existing_record["LocusStructure"])
                    #if abs(overlapping_interval.length() - (end_1based - start_0based)) < len(existing_record_motifs[0]):
                    #    set_value_if_not_yes(outer_join_overlap_table[overlapping_record["ChromStartEndLocusStruct"]], catalog_name, "Yes")
                    #    set_value_if_not_yes(outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]], overlapping_record["Source"], "Yes")
                    #### COMMENTED OUT because it's better to preprocess the input catalogs to trim all loci, then to do it on
                    #    the fly here, and so create redundant entries in the output table

                    if overlapping_interval.begin == start_0based and overlapping_interval.end == end_1based:
                        # need this because the loci might have different LocusIds in the input catalogs but the exact same start and end
                        set_value_if_not_yes(outer_join_overlap_table[overlapping_record["ChromStartEndLocusStruct"]], catalog_name, "Yes")
                        set_value_if_not_yes(outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]], overlapping_record["Source"], "Yes")
                    elif overlapping_interval.length() == (end_1based - start_0based):
                        set_value_if_not_yes(outer_join_overlap_table[overlapping_record["ChromStartEndLocusStruct"]], catalog_name, "YesButShifted")
                        set_value_if_not_yes(outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]], overlapping_record["Source"], "YesButShifted")
                    elif overlapping_interval.length() < (end_1based - start_0based):
                        set_value_if_not_yes(outer_join_overlap_table[overlapping_record["ChromStartEndLocusStruct"]], catalog_name, "YesButWider")
                        set_value_if_not_yes(outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]], overlapping_record["Source"], "YesButNarrower")
                    elif overlapping_interval.length() > (end_1based - start_0based):
                        set_value_if_not_yes(outer_join_overlap_table[overlapping_record["ChromStartEndLocusStruct"]], catalog_name, "YesButNarrower")
                        set_value_if_not_yes(outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]], overlapping_record["Source"], "YesButWider")


        if remove_existing:
            existing_interval["Discarded"] = True

        if discard_new:
            # mark this record as discarded and go to the next record in the current catalog
            new_record["Discarded"] = True
        else:
            counters["added"] += 1
            if write_bed_files_with_unique_loci:
                motifs = parse_motifs_from_locus_structure(new_record["LocusStructure"])
                if len(motifs) > 1:
                    print(f"Unexpected LocusStructure in {new_record}: {new_record['LocusStructure']}. Will not save to unique loci...")
                else:
                    unique_loci.add((unmodified_chrom, start_0based, end_1based, motifs[0]))

        interval_trees[chrom].add(intervaltree.Interval(start_0based, end_1based, data=new_record))

    print(f"Kept {counters['added']:,d} out of {counters['total']:,d} ({counters['added']/(counters['total'] or 1):6.1%}) "
          f"records from {variant_catalog_filename}")
    if verbose:
        for k, v in sorted(counters.items()):
            if k not in {"added", "total"}:
                print(" "*3, f"Discarded {counters[k]:7,d} out of {counters['total']:7,d} "
                      f"({counters[k]/counters['total']:6.1%}) records since they {k}")

    if stats is not None:
        stats[variant_catalog_filename] = counters

    if write_bed_files_with_unique_loci:
        unqiue_loci_bed_prefix = f"{output_prefix}." if output_prefix else ""
        unqiue_loci_bed_prefix += re.sub("(.json|.bed)(.gz)?$", "", os.path.basename(variant_catalog_json_or_bed))
        unique_loci_bed_filename = f"{unqiue_loci_bed_prefix}.unique_loci.bed"
        with open(unique_loci_bed_filename, "wt") as unique_loci_bed:
            for chrom, start_0based, end_1based, motif in sorted(unique_loci):
                unique_loci_bed.write("\t".join(map(str, [
                    chrom,
                    start_0based,
                    end_1based,
                    motif,
                    ".",
                ])) + "\n")

        os.system(f"bgzip -f {unique_loci_bed_filename}")
        os.system(f"tabix -f {unique_loci_bed_filename}.gz")
        print(f"Wrote {counters['added']:,d} unique loci from {variant_catalog_filename} to {unique_loci_bed_filename}.gz")

def check_whether_to_merge_adjacent_loci(
    previous_interval, current_interval, add_source_field=False):
    """Checks whether the two loci should be merged into a single locus.

    Args:
        previous_interval (intervaltree.Interval): The previous locus
        current_interval (intervaltree.Interval): The current locus
        add_source_field (bool): see the --add-source-field option
    Return:
        bool: True if the two loci should be merged into a single locus
        intervaltree.Interval: The merged locus if the two loci should be merged, otherwise None
    """

    # check whether the repeats are within 1bp of each other
    if previous_interval.end + 1 != current_interval.begin:
        return False, None

    # check if their motif is the same.
    previous_motifs = parse_motifs_from_locus_structure(previous_interval.data["LocusStructure"])
    if len(previous_motifs) != 1:
        # can't merge loci with multiple motifs
        return False, None

    current_motifs = parse_motifs_from_locus_structure(current_interval.data["LocusStructure"])
    if len(current_motifs) != 1:
        # can't merge loci with multiple motifs
        return False, None

    previous_motif = previous_motifs[0]
    previous_motif_shifted = compute_canonical_motif(previous_motifs[0], include_reverse_complement=False)
    current_motif_shifted = compute_canonical_motif(current_motifs[0], include_reverse_complement=False)
    if previous_motif_shifted != current_motif_shifted:
        return False, None

    # the loci have the same motif and are 1bp appart, so create a merged Interval
    unmodified_chrom, _, _ = parse_interval(previous_interval.data["ReferenceRegion"])
    chrom = unmodified_chrom.replace("chr", "")
    start_0based = previous_interval.begin
    end_1based = current_interval.end
    locus_structure = previous_interval.data["LocusStructure"]
    merged_data = {
        "LocusId": f"{chrom}-{start_0based}-{end_1based}-{previous_motif}",
        "ReferenceRegion": f"{unmodified_chrom}:{start_0based}-{end_1based}",
        "LocusStructure": locus_structure,
        "VariantType": "Repeat",
    }

    if add_source_field:
        if not previous_interval.data.get("Source"):
            raise ValueError(f"'Source' field not found in record {previous_interval.data}")
        if not current_interval.data.get("Source"):
            raise ValueError(f"'Source' field not found in record {current_interval.data}")

        if previous_interval.data["Source"] == current_interval.data["Source"]:
            merged_data["Source"] = previous_interval.data["Source"]
        else:
            merged_data["Source"] = previous_interval.data["Source"] + SEPARATOR_FOR_MULTIPLE_SOURCES + current_interval.data["Source"]

    merged_interval = intervaltree.Interval(
        previous_interval.begin,
        current_interval.end,
        data=merged_data,
    )

    #print("Merged",
    #      previous_interval.data["ReferenceRegion"],  previous_interval.data["LocusStructure"], "and",
    #      current_interval.data["ReferenceRegion"], current_interval.data["LocusStructure"], "into",
    #      merged_interval.data["ReferenceRegion"], merged_interval.data["LocusStructure"])

    return True, merged_interval


def convert_interval_trees_to_output_records(
    interval_trees,
    outer_join_overlap_table=None,
    only_loci_present_in_n_catalogs=None,
    merge_adjacent_loci_with_same_motif=False,
    add_source_field=False,
    add_found_in_fields=False,
):
    """Converts the IntervalTrees to a generator for output catalog records.

    Args:
        interval_trees (dict): a dictionary that maps chromosome names to IntervalTree objects for overlap detection
        outer_join_overlap_table (dict): a dictionary that maps locus IDs to a dictionary of catalog name to locus presence
        only_loci_present_in_n_catalogs (int): only output loci that are present in this many catalogs
        merge_adjacent_loci_with_same_motif (bool): see the --merge-adjacent-loci-with-same-motif option
        add_source_field (bool): see the --add-source-field option
        add_found_in_fields (bool): see the --add-found-in-fields option
    Yield:
        dict: A dictionary representing a record in the output catalog
    """
    counter = 0

    if not merge_adjacent_loci_with_same_motif:
        for interval_tree in interval_trees.values():
            for interval in sorted(interval_tree, key=lambda i: (i.begin, i.end)):
                new_record = interval.data
                if new_record.get("Discarded"):
                    continue

                if add_found_in_fields:
                    for catalog_name, value in outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]].items():
                        new_record[f"FoundIn{catalog_name}"] = value

                if only_loci_present_in_n_catalogs is not None and len(outer_join_overlap_table[new_record["ChromStartEndLocusStruct"]].values()) < only_loci_present_in_n_catalogs:
                    continue

                counter += 1
                yield new_record
    else:
        merged_counter = 0
        for interval_tree in interval_trees.values():
            previous_interval = None
            for interval in sorted(interval_tree, key=lambda i: (i.begin, i.end)):
                counter += 1
                if previous_interval is None:
                    previous_interval = interval
                    continue

                should_merge, merged_interval = check_whether_to_merge_adjacent_loci(
                    previous_interval, interval, add_source_field=add_source_field)
                if should_merge:
                    merged_counter += 1
                    previous_interval = merged_interval
                    continue

                new_record = previous_interval.data
                if new_record.get("Discarded"):
                    continue

                yield new_record
                previous_interval = interval

            if previous_interval is not None:
                yield previous_interval.data

        print(f"Merged {merged_counter:,d} out of {counter:,d} ({merged_counter/counter:6.1%}) adjacent loci that "
              f"were within 1bp of each other and had the same motif after cyclic shift")


def write_output_catalog(output_catalog_record_iter, output_path, output_format):
    """Writes the output catalog to a file.

    Args:
        output_catalog_record_iter (iter): An iterator (in sorted genomic order) over records to include in the
            output catalog. Each record is a dictionary in the ExpansionHunter variant catalog format.
        output_path (str): path of the output file
        output_format (str): either "JSON" or "BED"
    """

    # write the output catalog to the output file in the requested format
    if output_format == "JSON":
        output_catalog_record_list = []
        for record in output_catalog_record_iter:
            record = dict(record)
            del record["ChromStartEndLocusStruct"]
            output_catalog_record_list.append(record)

        fopen = gzip.open if output_path.endswith("gz") else open
        with fopen(output_path, "wt") as output_catalog:
            json.dump(output_catalog_record_list, output_catalog, indent=4, ignore_nan=True)
            #json.dump(output_catalog_record_iter, output_catalog, indent=4, ignore_nan=True)
        print(f"Wrote {len(output_catalog_record_list):,d} output records to {output_path}")
        #print(f"Wrote output records to {output_path}")

    elif output_format == "BED":
        output_path = re.sub(".b?gz$", "", output_path)
        with open(output_path, "wt") as output_catalog:
            total = 0
            for bed_record in sorted(convert_json_records_to_bed_format_tuples(output_catalog_record_iter)):
                total += 1
                output_catalog.write("\t".join(map(str, bed_record)) + "\n")

        os.system(f"bgzip -f {output_path}")
        os.system(f"tabix -f -p bed {output_path}.gz")
        print(f"Wrote {total:,d} output records to {output_path}.gz")


def print_catalog_stats(interval_trees):
    total = 0
    motif_counters = collections.defaultdict(int)
    source_counters = collections.defaultdict(int)
    chrom_counters = collections.defaultdict(int)
    for chrom, interval_tree in interval_trees.items():
        for interval in interval_tree:
            if interval.data.get("Discarded"):
                continue
            chrom_type = chrom.upper() if any(c in chrom.upper() for c in "XYM") else "autosomes"
            chrom_counters[chrom_type] += 1
            total += 1
            motifs = parse_motifs_from_locus_structure(interval.data["LocusStructure"])
            for motif in motifs:
                motif_label = f"{len(motif)}bp" if len(motif) <= 6 else f"7+bp"
                motif_counters[motif_label] += 1
            if "Source" in interval.data:
                source_counters[interval.data["Source"]] += 1

    if source_counters:
        print("Source of loci in output catalog:")
        for label, count in sorted(source_counters.items(), key=lambda x: x[1], reverse=True):
            print(f"   {count:9,d} out of {total:9,d} ({count/total:5.1%}) {label}")

    print("Motif sizes:")
    for label, count in sorted(motif_counters.items(), key=lambda x: x[1], reverse=True):
        print(f"   {count:9,d} out of {total:9,d} ({count/total:5.1%}) {label}")

    print("Chromsomes:")
    for label, count in sorted(chrom_counters.items(), key=lambda x: x[1], reverse=True):
        print(f"   {count:9,d} out of {total:9,d} ({count/total:5.1%}) are on {label}")


def replace_separator_for_multiple_entries_in_field(iterator, field_name="Source"):
    for record in iterator:
        if field_name not in record:
            raise ValueError(f"Unexpected absence of '{field_name}' field in record: {record}")

        if SEPARATOR_FOR_MULTIPLE_SOURCES in record[field_name]:
            record[field_name] = ", ".join(sorted(set(record[field_name].split(SEPARATOR_FOR_MULTIPLE_SOURCES))))

        yield record

def main():
    args, paths = parse_args()

    if not args.output_prefix:
        args.output_prefix = f"{args.merge_type}.{len(paths)}_catalogs"

    stats = collections.Counter()

    # parse each catalog and add each new unique record to this IntervalTrees dictionary
    interval_trees = collections.defaultdict(intervaltree.IntervalTree)

    outer_join_overlap_table = collections.defaultdict(dict)  # for each locus found in any catalog, maps its LocusId to a dictionary of catalog name to locus presence
    for i, (catalog_name, path, file_type) in enumerate(paths):
        add_variant_catalog_to_interval_trees(
            catalog_name, path, file_type, interval_trees, outer_join_overlap_table,
            overlapping_loci_action=args.overlapping_loci_action,
            min_overlap_fraction=args.overlap_fraction,
            motif_match_type=args.motif_match_type,
            discard_extra_fields_from_input_catalogs=args.discard_extra_fields_from_input_catalogs,
            add_source_field=args.add_source_field or args.add_found_in_fields or args.write_outer_join_table,
            stats=stats,
            verbose=args.verbose,
            verbose_overlaps=args.verbose_overlaps,
            show_progress_bar=args.show_progress_bar,
            write_bed_files_with_unique_loci=args.write_bed_files_with_unique_loci and i > 0,
            output_prefix=args.output_prefix,
        )

    # write the output catalog to a file
    output_formats = [args.output_format] if args.output_format else ["JSON", "BED"]

    for output_format in output_formats:
        output_catalog_record_generator = convert_interval_trees_to_output_records(
            interval_trees,
            outer_join_overlap_table=outer_join_overlap_table,
            only_loci_present_in_n_catalogs=len(paths) if args.merge_type == "intersection" else None,
            merge_adjacent_loci_with_same_motif=args.merge_adjacent_loci_with_same_motif,
            add_source_field=args.add_source_field,
            add_found_in_fields=args.add_found_in_fields)

        if args.add_source_field:
            # convert the SEPARATOR_FOR_MULTIPLE_SOURCES to commas
            output_catalog_record_generator = replace_separator_for_multiple_entries_in_field(
                output_catalog_record_generator, field_name="Source")


        output_path = f"{args.output_prefix}.{output_format.lower()}"
        if output_format == "JSON":
            output_path += ".gz"

        if args.verbose:
            print(f"Writing combined catalog to {output_path}")
        write_output_catalog(output_catalog_record_generator, output_path, output_format)

    if args.verbose:
        print_catalog_stats(interval_trees)

    if args.write_outer_join_table:
        outer_join_overlap_table_path = f"{args.output_prefix}.outer_join_overlap_table.tsv.gz"
        output_counter = 0
        with gzip.open(outer_join_overlap_table_path, "wt") as outer_join_overlap_table_tsv:
            header_string = "LocusId\t" + "\t".join([catalog_name for catalog_name, _, _ in paths])
            outer_join_overlap_table_tsv.write(f"{header_string}\n")
            for locus_id, catalog_presence in sorted(outer_join_overlap_table.items()):
                output_counter += 1
                if sum(1 for catalog_name, _, _ in paths if catalog_name in catalog_presence) >= args.outer_join_overlap_table_min_sources:
                    row_string = f"{locus_id}\t" + "\t".join(catalog_presence.get(catalog_name, "") for catalog_name, _, _ in paths)
                    outer_join_overlap_table_tsv.write(f"{row_string}\n")
        print(f"Wrote {output_counter:,d} rows to {outer_join_overlap_table_path}")

    if args.write_merge_stats_tsv:
        merge_stats_output_tsv_path = f"{args.output_prefix}.merge_stats.tsv"
        with open(merge_stats_output_tsv_path, "wt") as merge_stats_tsv:
            merge_stats_tsv.write("Catalog\t"
                                  "Added\t"
                                  "Total\t"
                                  "Overlapped an existing locus and had the exact same LocusStructure\t"
                                  "Overlapped an existing locus and had the same canonical motif\t"
                                  "Overlapped an existing locus and one motif was contained within the other\n")
            output_counter = 0
            for catalog, catalog_stats in stats.items():
                output_row = [catalog, catalog_stats["added"], catalog_stats["total"]]
                for key_suffix in "had the exact same LocusStructure", "had the same canonical motif", "one motif was contained within the other":
                    keys = [k for k in catalog_stats.keys() if k.endswith(key_suffix)]
                    if len(keys) == 0:
                        value = 0
                    elif len(keys) == 1:
                        value = catalog_stats.get(keys[0])
                    else:
                        raise ValueError(f"Unexpected stats keys: {keys}")
                    output_row.append(value)
                output_counter += 1
                merge_stats_tsv.write("\t".join(map(str, output_row)) + "\n")
            print(f"Wrote {output_counter:,d} rows to {merge_stats_output_tsv_path}")

if __name__ == "__main__":
    main()
