"""This script makes it easier to manually look through genotypes of many samples at known pathogenic loci and
check for samples that may be expanded into the pathogenic range.

The script's main input file is a .tsv file generated by the combine_expansion_hunter_json_to_tsv.py script which
combines multiple ExpansionHunter .json output files into a single .tsv table. It's assumed that this input .tsv will
have at least the columns listed below. One way to add these Sample_* and VariantCatalog_* columns is to run
combine_expansion_hunter_json_to_tsv.py with the --sample-metadata and the --variant-catalog args.

The input .tsv must have at least these input columns:

    "LocusId", "SampleId", "Sample_affected", "Sample_sex",
    "Num Repeats: Allele 1", "Num Repeats: Allele 2", "CI end: Allele 1", "CI end: Allele 2",

"""

import argparse
import os
import subprocess
from ast import literal_eval

import pandas as pd
from tabulate import tabulate


REQUIRED_COLUMNS = [
    "LocusId",
    "SampleId",
    "Sample_affected",
    "Sample_sex",
    "Num Repeats: Allele 1",
    "Num Repeats: Allele 2",
    "CI end: Allele 1",
    "CI end: Allele 2",
]

OPTIONAL_COLUMNS = [
    "RepeatUnit",
    "VariantId",

    "Sample_analysis_status",
    "Sample_coded_phenotype",
    "Sample_phenotypes",
    "Sample_genome_version",
    "Sample_sample_type",

    "VariantCatalog_Inheritance",
    "VariantCatalog_Diseases",

    "Genotype",
    "GenotypeConfidenceInterval",
]


def run(command):
    print(command)
    subprocess.check_call(command, shell=True)


def main():
    p = argparse.ArgumentParser()
    grp = p.add_mutually_exclusive_group(required=True)
    grp.add_argument("--use-affected", action="store_true", help="Use affected status to determine which samples to "
        "include in the output for each locus. Only include those affected samples that have longer expansions than "
        "the top 10 unaffected samples (adjustsable by --max-n-unaffected).")
    grp.add_argument("--use-thresholds", action="store_true", help="Use known pathogenic thresholds to determine which "
        "samples to include in the output for each locus. All samples with expansions above the pathogenic threshold "
        "will be included.")
    p.add_argument("-n", "--max-rows", type=int, default=10000000, help="Limit the max number of samples to include in "
        "the output for each locus.")
    p.add_argument("--max-n-unaffected", type=int, default=10, help="After this many unaffected samples are "
        "encountered with genotypes above a particular expansion size at locus, all samples (affected or unaffected) "
        "that have smaller expansions at that locus will be ignored")
    p.add_argument("-l", "--locus", action="append", help="If specified, only these locus ids will be processed")
    p.add_argument("combined_tsv_path", nargs="+", help="Path of combined ExpansionHunter .tsv table generated by the "
        "combine_expansion_hunter_json_to_tsv.py script. It's assumed that combine_expansion_hunter_json_to_tsv.py "
        "was run with --sample-metadata and --variant-catalog args to add sample-level and locus-level metadata columns")

    # Parse and validate command-line args + read in the combined table(s) from the given command_tsv_path(s)
    args = p.parse_args()

    all_dfs = []
    all_locus_ids = set()
    for combined_tsv_path in args.combined_tsv_path:
        if not os.path.isfile(combined_tsv_path):
            p.error(f"{combined_tsv_path} not found")

        df = pd.read_table(combined_tsv_path, low_memory=False)

        # check that all required columns are present
        missing_required_columns = set(REQUIRED_COLUMNS) - set(df.columns)
        if missing_required_columns:
            p.error(f"{combined_tsv_path} is missing these required columns: {missing_required_columns}")

        # fill in values for missing optional columns
        missing_optional_columns = set(OPTIONAL_COLUMNS) - set(df.columns)
        if missing_optional_columns:
            print(f"WARNING: {combined_tsv_path} is missing these columns: {missing_optional_columns}. "
                  f"Filling them with None...")
            for c in missing_optional_columns:
                df.loc[:, c] = None

        all_locus_ids |= set(df.LocusId)
        all_dfs.append((df, combined_tsv_path))

    # Process each locus
    for locus_id in sorted(all_locus_ids):
        if args.locus and locus_id not in args.locus:
            continue

        print(">"*100)  # print a divider
        for i, (full_df, df_source_path) in enumerate(all_dfs):

            print("="*100)  # print a divider
            print(f"** {locus_id} from {df_source_path} **")

            locus_specific_df = full_df[full_df.LocusId == locus_id]
            if len(locus_specific_df) == 0:
                continue

            locus_specific_df = locus_specific_df.sort_values(by=["Num Repeats: Allele 2", "Num Repeats: Allele 1"], ascending=False)

            # get the 1st row and use it to look up metadata values which are the same across all rows for the locus (eg. Inheritance Mode)
            first_row = locus_specific_df.iloc[0].to_dict()
            inheritance_mode = first_row.get("VariantCatalog_Inheritance") or "Unknown"

            disease_info = first_row.get("VariantCatalog_Diseases")
            intermediate_threshold_min = None
            pathogenic_threshold_min = None
            if disease_info:
                try:
                    disease_info = literal_eval(disease_info)
                except Exception as e:
                    raise ValueError(f"Unable to parse {disease_info} as json: {e}")

                try:
                    intermediate_threshold_min = min(int(float(d["NormalMax"]) + 1) for d in disease_info if d.get("NormalMax"))
                except ValueError as e:
                    print(f"WARNING: {locus_id} couldn't parse NormalMax fields from disease_info {disease_info}: {e}")

                try:
                    pathogenic_threshold_min = min(int(float(d["PathogenicMin"])) for d in disease_info if d.get("PathogenicMin"))
                except ValueError as e:
                    print(f"WARNING: {locus_id} couldn't parse PathogenicMin fields from disease_info {disease_info}: {e}")

            reference_region = first_row["ReferenceRegion"]
            genome_version = f"GRCh{first_row['Sample_genome_version']}" if first_row.get('Sample_genome_version') else ""
            motif = first_row.get("RepeatUnit")
            locus_description = f"{locus_id} ({reference_region}: {genome_version})  https://stripy.org/database/{locus_id}"
            print("**Locus**: ", locus_description)
            print("**Disease**: ", str(disease_info))
            print("**Inheritance**: ", inheritance_mode)
            print("**Pathogenic Threshold**: >=", pathogenic_threshold_min, "x", motif)
            print("**Intermediate Threshold**: >=", intermediate_threshold_min, "x", motif)

            locus_specific_df.loc[:, "Sample_sample_type"] = locus_specific_df["Sample_sample_type"].fillna("Unknown")
            locus_specific_df.loc[:, "Sample_sex"] = locus_specific_df["Sample_sex"].fillna("Unknown")
            locus_specific_df.loc[:, "Sample_affected"] = locus_specific_df["Sample_affected"].fillna("Unknown")

            filtered_df_list = []
            for sample_type in sorted(set(locus_specific_df["Sample_sample_type"])):
                locus_and_sample_type_specific_df = locus_specific_df[
                    locus_specific_df["Sample_sample_type"].str.contains(sample_type) |
                    locus_specific_df["Sample_sample_type"].str.contains("Unknown")
                ]

                if locus_id == "COMP":
                    # COMP is a special case where contractions below 5 repeats are also pathogenic
                    df_comp = locus_and_sample_type_specific_df[
                        (locus_and_sample_type_specific_df["CI end: Allele 1"] < 5) |
                        (locus_and_sample_type_specific_df["CI end: Allele 2"] < 5)
                    ]
                    filtered_df_list.append(df_comp)

                if args.use_thresholds:
                    threshold = intermediate_threshold_min or pathogenic_threshold_min
                else:
                    threshold = 0

                if threshold:
                    if inheritance_mode == "XR":
                        df_female = locus_and_sample_type_specific_df[
                            (locus_and_sample_type_specific_df["Sample_sex"].str.upper().str.startswith("F") | locus_and_sample_type_specific_df["Sample_sex"].str.upper().str.startswith("U")) &
                            (locus_and_sample_type_specific_df["CI end: Allele 1"] >= threshold) &
                            (locus_and_sample_type_specific_df["CI end: Allele 2"] >= threshold)
                        ].iloc[0:args.max_rows]
                        df_male = locus_and_sample_type_specific_df[
                            (locus_and_sample_type_specific_df["Sample_sex"].str.upper().str.startswith("M")) &
                            (locus_and_sample_type_specific_df["CI end: Allele 1"] >= threshold)
                        ].iloc[0:args.max_rows]
                        filtered_df_for_locus_and_sample_type = pd.concat([df_male, df_female])
                    elif inheritance_mode == "AR":
                        filtered_df_for_locus_and_sample_type = locus_and_sample_type_specific_df[
                            (locus_and_sample_type_specific_df["CI end: Allele 1"] >= threshold) &
                            (locus_and_sample_type_specific_df["CI end: Allele 2"] >= threshold)
                        ] .iloc[0:args.max_rows]
                    else:
                        filtered_df_for_locus_and_sample_type = locus_and_sample_type_specific_df[
                            (locus_and_sample_type_specific_df["CI end: Allele 1"] >= threshold) |
                            (locus_and_sample_type_specific_df["CI end: Allele 2"] >= threshold)
                        ].iloc[0:args.max_rows]

                    filtered_df_list.append(filtered_df_for_locus_and_sample_type)

                if args.use_affected:
                    if inheritance_mode == "XR":
                        dfs_to_process = [
                            locus_and_sample_type_specific_df[
                                locus_and_sample_type_specific_df["Sample_sex"].str.upper().str.startswith("F")
                            ],
                            locus_and_sample_type_specific_df[
                                locus_and_sample_type_specific_df["Sample_sex"].str.upper().str.startswith("M")
                            ],
                        ]
                    else:
                        dfs_to_process = [locus_and_sample_type_specific_df]

                    for locus_and_sample_type_and_sex_specific_df in dfs_to_process:

                        unaffected_counter = 0
                        idx = 0
                        for affected_status in locus_and_sample_type_and_sex_specific_df["Sample_affected"]:
                            idx += 1
                            if affected_status and ("Not Affected" in affected_status):
                                unaffected_counter += 1

                            if unaffected_counter >= args.max_n_unaffected:
                                break

                        filtered_df_for_locus_and_sample_type_and_sex = locus_and_sample_type_and_sex_specific_df.iloc[:idx]

                        filtered_df_list.append(filtered_df_for_locus_and_sample_type_and_sex)

            filtered_df_for_locus = pd.concat(filtered_df_list)
            print(f"Found {len(filtered_df_for_locus)} samples passed filters")

            if len(filtered_df_for_locus) == 0:
                continue

            filtered_df_for_locus = filtered_df_for_locus.sort_values(by=["Num Repeats: Allele 2", "Num Repeats: Allele 1"], ascending=False)

            # Print the filtered results for this locus
            filtered_df_for_locus = filtered_df_for_locus[[
                "SampleId",
                "LocusId",
                "VariantId",

                "Genotype",
                "GenotypeConfidenceInterval",

                "Sample_sex",
                "Sample_affected",
                "Sample_sample_type",

                "Sample_analysis_status",
                "Sample_coded_phenotype",
                "Sample_genome_version",

                "RepeatUnit",
                "VariantCatalog_Inheritance",
            ]]

            # Shorten some column names so more can fit on screen
            filtered_df_for_locus.rename(columns={
                "Sample_sample_sex": "sex",
                "Sample_sample_type": "type",
                "GenotypeConfidenceInterval": "GenotypeCI",
            }, inplace=True)

            # Print the candidate pathogenic rows for this locus
            print(tabulate(filtered_df_for_locus, headers="keys", tablefmt="psql", showindex=False))


if __name__ == "__main__":
    main()
